<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GSoC2021 | Larry Dong</title>
    <link>https://www.larrydong.com/gsoc2021/</link>
      <atom:link href="https://www.larrydong.com/gsoc2021/index.xml" rel="self" type="application/rss+xml" />
    <description>GSoC2021</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2021 Larry Dong</copyright>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>GSoC2021</title>
      <link>https://www.larrydong.com/gsoc2021/</link>
    </image>
    
    <item>
      <title>ðŸ“š Diving into the math...</title>
      <link>https://www.larrydong.com/gsoc2021/community-bonding-week-2/</link>
      <pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate>
      <guid>https://www.larrydong.com/gsoc2021/community-bonding-week-2/</guid>
      <description>&lt;p&gt;My second week was spent mostly diving into textbooks to better understand what Dirichlet processes entail. To do so, I started with a brief revision of probability theory and reviewing Gaussian processes. Why? Given that I &lt;del&gt;am&lt;/del&gt; should be studying for my comprehensive exams, I thought that I better understand what it means to posit priors on the space of probability measures. As recommended by my mentors, a good starting point would be to look at Gaussian processes (GP) since I found Dirichlet processes very difficult to wrap my head around. GPs are convenient priors for continuous functions and they seem to be slightly more easy to understand.&lt;/p&gt;
&lt;p&gt;To document my learning process of statistical theory, I intend to summarize some key theoretical underpinnings in this blog. We&amp;rsquo;ll see where this goes! In light of all the math below, here one of my favorite Calvin and Hobbes strip.&lt;/p&gt;
&lt;img src=&#34;Calvin-and-Hobbes-Leave-Math-to-The-Machines.jpeg&#34; width=&#34;65%&#34;&gt;
&lt;p&gt;As a side note, I am part of the organizing committee for the 2021 Canadian Statistics Student Conference which will be happening this Saturday, June 5. Quebec also just announced that the administration of second vaccination doses have been sped up. All lot is going on, and it&amp;rsquo;s all exciting!&lt;/p&gt;
&lt;h2 id=&#34;1---probability-theory&#34;&gt;1 - Probability Theory&lt;/h2&gt;
&lt;p&gt;This section follows closely Chapters 1 - 3 from 
&lt;a href=&#34;http://static.stevereads.com/papers_to_read/probability_with_martingales_williams_.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Probability with Martingales&lt;/a&gt;. For a more in-depth read, 
&lt;a href=&#34;https://www.colorado.edu/amath/sites/default/files/attached-files/billingsley.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Probability and Measures&lt;/a&gt; sounds great but challenging. The main reason why I go through some (probably way too dense/deep) theory before jumping into Dirichlet processes (DP) is because DPs is a suitable prior over the space of probability measures.&lt;/p&gt;
&lt;h4 id=&#34;11---what-is-a-measure&#34;&gt;1.1 - What is a measure?&lt;/h4&gt;
&lt;p&gt;Formally, we start with $(S, \Sigma)$ be a measurable space where $S$ is some topological space and $\Sigma$ is the $\sigma$-algebra generated by open subsets of $S$. A mapping $\mu: \Sigma \rightarrow [0, +\infty]$ on $(S, \Sigma)$ is a &lt;em&gt;measure&lt;/em&gt; if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(1) $\mu(A) \geq 0$ for all $A \in \Sigma$ and;&lt;/li&gt;
&lt;li&gt;(2) $\mu$ is countably additive, i.e. $\mu\left(\cup_{i=1}^\infty A_i\right) = \sum_{i=1}^\infty \mu(A_i)$ and $\mu(\emptyset) = 0$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Transitioning from general, super abstract sets, $\sigma$-algebras and crazy measures, we usually define a probability triple $(\Omega, \mathcal{F}, \mathbb{P})$ as followed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Omega$: sample space;&lt;/li&gt;
&lt;li&gt;$\mathcal{F}$: set of possible events $E \subseteq \Omega$ (and is a $\sigma$-algebra);&lt;/li&gt;
&lt;li&gt;$\mathbb{P}$: probability measure, i.e. a measure $\mu$ such that $\mu(\Omega) = 1$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another more intuitive way we can envision measurable spaces is as $(\Omega, \mathcal{F}) \stackrel{\text{in words}}{=}$ (sample space, family of events).&lt;/p&gt;
&lt;h4 id=&#34;12---what-is-a-random-variable&#34;&gt;1.2 - What is a random variable?&lt;/h4&gt;
&lt;p&gt;In statistics, we are interested in random variables and their properties. Random variables assign values to possible events in $\mathcal{F}$ on which probability functions $\mathbb{P}$ can be evaluated. Given a measurable space $(S, \Sigma)$, a class of functions $h: S \rightarrow \mathbb{R}$ is $\Sigma$-measurable if, for all $A \in \mathcal{B}$, its inverse $h^{-1}: \mathcal{B} \rightarrow \Sigma$ has the following property:&lt;/p&gt;
&lt;p&gt;\begin{align*}
h^{-1}(A) = \{s \in S \,\vert\,  h(s) \in A \}
\end{align*}&lt;/p&gt;
&lt;p&gt;where $\mathcal{B} := \sigma$(open sets of $\mathbb{R}$) is the Borel set.&lt;/p&gt;
&lt;p&gt;With this, a random variable is &amp;ldquo;simply&amp;rdquo; defined to be a mapping $X: \Omega \rightarrow \mathbb{R}$ that is $\Sigma$-measurable.&lt;/p&gt;
&lt;h4 id=&#34;13---summary-of-probability-theory-already&#34;&gt;1.3 - Summary of probability theory (already?)&lt;/h4&gt;
&lt;p&gt;The overarching idea is that, for a probability space (also called probability triple) $(\Omega, \mathcal{F}, \mathbb{P})$, we have the following mappings outlined:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\Omega &amp;amp;\stackrel{X}{\rightarrow} \mathbb{R}\\\&lt;br&gt;
\mathcal{B} \stackrel{X^{-1}}{\rightarrow} &amp;amp; \mathcal{F} \stackrel{\mathbb{P}}{\rightarrow} [0, 1] .
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;2---bayesian-nonparametrics&#34;&gt;2 - Bayesian nonparametrics&lt;/h2&gt;
&lt;p&gt;When performing inference, we often posit parametric assumptions on the data-generating mechanism. However, this can be restrictive especially when we don&amp;rsquo;t know the functional form of such underlying mechanism. In these situations, we can turn to nonparametric methods which, as the name suggests, do &lt;strong&gt;not&lt;/strong&gt; posit distributional assumptions and hence protects against model misspecification. In Bayesian nonparametrics (BNP), we can put priors on functions or probability distribution themselves. As Peter Mueller once said when talking about ANOVA DDPs, a type of Dirichlet process: &amp;ldquo;A BNP is always right in the sense, no matter the true distribution, our prior always puts some probability mass in some neighborhood of the truth so we can learn about it. It has full support.&amp;rdquo; (see 
&lt;a href=&#34;https://youtu.be/qGZrkiLLhe4?t=2771&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;video&lt;/a&gt; here)&lt;/p&gt;
&lt;h4 id=&#34;21---gaussian-processes&#34;&gt;2.1 - Gaussian processes&lt;/h4&gt;
&lt;p&gt;By definition, a GP is a stochastic process in which any finite collection of observations $\mathcal{D} = \{(x_i, y_i = f(x_i) \}_{i=1}^N$ follows a multivariate normal (MVN) distribution with $N$-dimensional mean $\mu(\pmb{x})$ and a $N \times N$ covariance matrix $\kappa$ where the $i$, $j$th entry can be specified as followed:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\kappa_{ij} = \kappa(x_i, x_j) = \sigma^2_f \,\text{exp} \Big\{-\frac{1}{2\ell^2} (x_i - x_j)^2 \Big\}
\end{align*}&lt;/p&gt;
&lt;p&gt;for some $(\sigma^2_f, \ell^2) \in \mathbb{R}_+^2$. It is to my understanding that $\mu(\pmb{x})$ is often assumed to be $\vec{0}$ &lt;sub&gt;&lt;sup&gt;(for reasons that are unknown to me at the moment)&lt;/sup&gt;&lt;/sub&gt;.&lt;/p&gt;
&lt;p&gt;An interesting way of using GPs as prior distributions is that, say we have $N$ observations $(x_1, f(x_1)), \dots, (x_N, f(x_N))$, we want to perform inference on the unknown function $f$ &lt;em&gt;without&lt;/em&gt; any making any assumptions on its functional form. Because any collection of observations $\sim$ MVN, we are able to form predictions for $f(x_\star)$ for any unobserved $x_\star$. This stems from a well-established property of MVNs in that, if $(X_1, \dots, X_N) \sim \text{MVN}$, $\vec{\pmb{X}}_{\mathcal{I}_1} \, | \, \vec{\pmb{X}}_{\mathcal{I}_2} \sim \text{MVN}$ where $\mathcal{I}_1 \cup \mathcal{I}_2 = \{1, \dots, n\}$ and $\mathcal{I}_1 \cap \mathcal{I}_2 = \emptyset$. Using this, we have that:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\Big\{f(x_\star) \, | \, x_\star, \pmb{x}, f(\pmb{x})\Big\} &amp;amp;\sim \text{MVN}(\mu_\star, \Sigma_\star)\\\&lt;br&gt;
\text{where } \mu_\star &amp;amp;= \mu(\pmb{x}) + \kappa_\star^\text{T} \kappa^{-1}\big(f(\pmb{x}) - \mu(\pmb{x})\big)\\\&lt;br&gt;
\kappa_\star &amp;amp;= \kappa_{\star \star} - \kappa_{\star}^\text{T} \kappa^{-1} \kappa_{\star}
\end{align*}&lt;/p&gt;
&lt;p&gt;where $\kappa_{\star} = \kappa(\pmb{x}, x_\star)$ and $\kappa_{\star\star} = \kappa(x_\star, x_\star)$.&lt;/p&gt;
&lt;h6 id=&#34;brief-simulation&#34;&gt;Brief simulation&lt;/h6&gt;
&lt;p&gt;To visualize the results above, I wrote a short notebook that shows the 95% equi-tailed credible intervals for all $x$-values. Code is available 
&lt;a href=&#34;https://github.com/larryshamalama/pymc3-playground/blob/master/gaussian-processes.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/p&gt;
&lt;img src=&#34;gp-simulation.png&#34; width=&#34;75%&#34;&gt;
&lt;h4 id=&#34;22---dirichlet-processes&#34;&gt;2.2 - Dirichlet processes&lt;/h4&gt;
&lt;p&gt;Stay tuned, I hope to talk about DPs in my next GSoC blogpost!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ðŸš€ End of a Remote First Year PhD, Beginning of GSoC2021</title>
      <link>https://www.larrydong.com/gsoc2021/community-bonding-week-1/</link>
      <pubDate>Thu, 20 May 2021 00:00:00 +0000</pubDate>
      <guid>https://www.larrydong.com/gsoc2021/community-bonding-week-1/</guid>
      <description>&lt;p&gt;The first year of my PhD studies had, in all honesty, some rather unexpected challenges. Given the ongoing pandemic, my move to a new city for my PhD studies was put on hold. Instead, I decided to spend more quality time with one of my good friends (he&amp;rsquo;s a great cook!) and my parents. As spring was looming in, I quickly ran out of motivation to work on assignments and write my end-of-term report, but, with the help of my support network and all things considered, my first year ended well!&lt;/p&gt;
&lt;p&gt;With summer right around the corner, I was eager for some good news and an opportunity to look forward to. On Monday, May 17 right before 2pm, I received the following email and nearly floored with happiness:&lt;/p&gt;
&lt;img src=&#34;acceptance.png&#34; width=&#34;75%&#34;&gt;
&lt;p&gt;and I had to immediately share this amazing news with my parents and my friends! I am extremely grateful for the Google Summer of Code (GSoC) opportunity for the many reasons. Firstly, I will learn more about open source development and probabilistic programming from well-established mentors. Secondly, this practical opportunity would complement well my summer studying for my PhD comprehensive exams. Last but not least, I&amp;rsquo;m being paid! ðŸ˜…&lt;/p&gt;
&lt;p&gt;Under the mentorship of 
&lt;a href=&#34;https://twitter.com/fonnesbeck&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Christopher Fonnesbeck&lt;/a&gt; and 
&lt;a href=&#34;https://www.austinrochford.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Austin Rochford&lt;/a&gt;, my GSoC project centers around extending the 
&lt;a href=&#34;https://github.com/pymc-devs/pymc3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyMC3&lt;/a&gt; package with a Dirichlet process submodule. PyMC3 is a probabilistic framework in Python that allows users to fit Bayesian models via MCMC sampling. There is a growing interest in Bayesian nonparametric methods for fitting statistical models without specifying its parametric form. Dirichlet processes can be used as priors for probability distributions themselves. How exactly? I shall find out in the next three weeks of community bonding!&lt;/p&gt;
&lt;p&gt;In the coming months, I look forward to getting to know the PyMC3 community, reading more about Bayesian nonparametric statistics and, above all, learning from my mentors. Stay tuned for more!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
