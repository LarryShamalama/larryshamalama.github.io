<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GSoC2021 | Larry Dong</title>
    <link>https://www.larrydong.com/gsoc2021/</link>
      <atom:link href="https://www.larrydong.com/gsoc2021/index.xml" rel="self" type="application/rss+xml" />
    <description>GSoC2021</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2021 Larry Dong</copyright>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>GSoC2021</title>
      <link>https://www.larrydong.com/gsoc2021/</link>
    </image>
    
    <item>
      <title>ðŸ“š Diving into the math...</title>
      <link>https://www.larrydong.com/gsoc2021/community-bonding-week-2/</link>
      <pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate>
      <guid>https://www.larrydong.com/gsoc2021/community-bonding-week-2/</guid>
      <description>&lt;h2 id=&#34;1---probability-theory&#34;&gt;1 - Probability Theory&lt;/h2&gt;
&lt;p&gt;This section follows closely Chapters 1 - 3 from 
&lt;a href=&#34;http://static.stevereads.com/papers_to_read/probability_with_martingales_williams_.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Probability with Martingales&lt;/a&gt; by David Williams. For a more in-depth read, 
&lt;a href=&#34;https://www.colorado.edu/amath/sites/default/files/attached-files/billingsley.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Probability and Measures&lt;/a&gt; by Patrick Billingsley sounds great but challenging.&lt;/p&gt;
&lt;h4 id=&#34;11---what-is-a-measure&#34;&gt;1.1 - What is a measure?&lt;/h4&gt;
&lt;p&gt;Formally, we start with $(S, \Sigma)$ be a measurable space where $S$ is some topological space and $\Sigma$ is the $\sigma$-algebra generated by open subsets of $S$. A mapping $\mu: \Sigma \rightarrow [0, +\infty]$ on $(S, \Sigma)$ is a &lt;em&gt;measure&lt;/em&gt; if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(1) $\mu(A) \geq 0$ for all $A \in \Sigma$ and;&lt;/li&gt;
&lt;li&gt;(2) $\mu$ is countably additive.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Transitioning from general, super abstract sets, $\sigma$-algebras and crazy measures, we usually define a probability triple $(\Omega, \mathcal{F}, \mathbb{P})$ as followed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\Omega$: sample space;&lt;/li&gt;
&lt;li&gt;$\mathcal{F}$: set of possible events $E \subseteq \Omega$ (and is a $\sigma$-algebra);&lt;/li&gt;
&lt;li&gt;$\mathbb{P}$: probability measure, i.e. a measure $\mu$ such that $\mu(\Omega) = 1$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The reason we go through general measures, ($\sigma$)-algebras and their properties is because they are applicable to $(\Omega, \mathcal{F}) \stackrel{\text{in words}}{=}$ (sample space, family of events), a measurable space, but just less crazy.&lt;/p&gt;
&lt;h4 id=&#34;12---what-is-a-random-variable&#34;&gt;1.2 - What is a random variable?&lt;/h4&gt;
&lt;p&gt;In statistics, we are interested in random variables and their properties. Random variables assign values to possible events in $\mathcal{F}$ on which probability functions can be evaluated. Given a measurable space $(S, \Sigma)$, a class of functions $h: S \rightarrow \mathbb{R}$ is $\Sigma$-measurable if, for all $A \in \mathcal{B}$, its inverse $h^{-1}: \mathcal{B} \rightarrow \Sigma$ has the following property:&lt;/p&gt;
&lt;p&gt;\begin{align*}
h^{-1}(A) = {s \in S \vert  h(s) \in A }
\end{align*}&lt;/p&gt;
&lt;p&gt;where $\mathcal{B} := \sigma$(open sets of $\mathbb{R}$) is the Borel set.&lt;/p&gt;
&lt;h4 id=&#34;13---summary-of-probability-theory-already&#34;&gt;1.3 - Summary of probability theory (already?)&lt;/h4&gt;
&lt;p&gt;The overarching idea is that, for a probability triple $(\Omega, \mathcal{F}, \mathbb{P})$, we have the following mappings outlined:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\Omega &amp;amp;\stackrel{X}{\rightarrow} \mathbb{R}\\\&lt;br&gt;
\mathcal{B} \stackrel{X^{-1}}{\rightarrow} &amp;amp; \mathcal{F} \stackrel{\mathbb{P}}{\rightarrow} [0, 1] .
\end{align*}&lt;/p&gt;
&lt;h2 id=&#34;2---bayesian-inference&#34;&gt;2 - Bayesian Inference&lt;/h2&gt;
&lt;h4 id=&#34;21---overview-of-the-bayesian-paradigm&#34;&gt;2.1 - Overview of the Bayesian Paradigm&lt;/h4&gt;
&lt;p&gt;In the frequentist framework, statistical inference is performed by solely looking at the likelihood function, defined to be the probability of observing the observed data and denoted by $\mathcal{L}(\theta; \pmb{x})$. We let $\theta \in \Theta \subseteq \mathbb{R}^p$ to be the $p$-dimensional parameter of interest and $\pmb{x} = (x_1, \dots, x_n)$ to be observed values or realisations of the random vector $\pmb{X} = (X_1, \dots, X_n)$. Often in mathematical statistics courses, $X_1, \dots, X_n \sim F(\cdot)$ are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(1) independent and identically distributed and;&lt;/li&gt;
&lt;li&gt;(2) $F(x; \theta)$ is a distribution &lt;em&gt;parametrized&lt;/em&gt; by $\theta$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the frequentist framework, statistical inference is performed by solely looking at the likelihood function, defined to be the probability of observing the observed data and denoted by $\mathcal{L}(\theta; \pmb{x})$:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\mathcal{L}(\theta; \pmb{x}) = \prod_{i=1}^n f(x_i; \theta)
\end{align*}&lt;/p&gt;
&lt;p&gt;where $f(\cdot; \theta)$ is the probability density function for $X$. The maximum likelihood estimator $\widehat{\theta} = \text{argmax}_{\theta \in \Theta} \mathcal{L}(\theta; \pmb{x})$ exhibits good asymptotic properties (or &amp;ldquo;mild&amp;rdquo;) conditions.&lt;/p&gt;
&lt;p&gt;In the &lt;strong&gt;Bayesian&lt;/strong&gt; paradigm for parametric models, we incorporate prior belief about $\theta$ into the our inference-based procedure. In other words, we are interested in a posterior distribution $\pi_n(\theta; \pmb{x})$ and we update our initial &amp;ndash; which can be close to no &amp;ndash; prior belief $\pi(\theta)$ with information from the observed data, i.e. the likelihood $\mathcal{L}(\theta; \pmb{x})$:&lt;/p&gt;
&lt;p&gt;\begin{align*}
\pi_n(\theta; \pmb{x}) \stackrel{\theta}{\propto} \mathcal{L}(\theta; \pmb{x}) \pi(\theta) .
\end{align*}&lt;/p&gt;
&lt;h4 id=&#34;22-what-are-bayesian-nonparametrics&#34;&gt;2.2 What are Bayesian nonparametrics?&lt;/h4&gt;
&lt;p&gt;From above, a slight discrepancy lies in the interpretation, estimation procedure and philosophy behind Bayesian and frequentist inference. However, to perform inference on $\theta$, we must first posit parametric assumptions on our hypothesized data generating process. In many situations, we may still address research or scientific questions without positing parametric assumptions. By doing so, we cut out the middle man in our statistical inference procedure and go &lt;em&gt;nonparametric&lt;/em&gt; ðŸ‘».&lt;/p&gt;
&lt;h4 id=&#34;23-exchangeability&#34;&gt;2.3 Exchangeability&lt;/h4&gt;
&lt;h2 id=&#34;3---dirichlet-processes&#34;&gt;3 - Dirichlet Processes&lt;/h2&gt;
&lt;h2 id=&#34;why-is-this-important&#34;&gt;WHY IS THIS IMPORTANT&lt;/h2&gt;
&lt;p&gt;That&amp;rsquo;s a great question, Charles-Auguste.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ðŸš€ End of a Remote First Year PhD, Beginning of GSoC2021</title>
      <link>https://www.larrydong.com/gsoc2021/community-bonding-week-1/</link>
      <pubDate>Thu, 20 May 2021 00:00:00 +0000</pubDate>
      <guid>https://www.larrydong.com/gsoc2021/community-bonding-week-1/</guid>
      <description>&lt;p&gt;The first year of my PhD studies had, in all honesty, some rather unexpected challenges. Given the ongoing pandemic, my move to a new city for my PhD studies was put on hold. Instead, I decided to spend more quality time with one of my good friends (he&amp;rsquo;s a great cook!) and my parents. As spring was looming in, I quickly ran out of motivation to work on assignments and write my end-of-term report, but, with the help of my support network and all things considered, my first year ended well!&lt;/p&gt;
&lt;p&gt;With summer right around the corner, I was eager for some good news and an opportunity to look forward to. On Monday, May 17 right before 2pm, I received the following email and nearly floored with happiness:&lt;/p&gt;
&lt;img src=&#34;acceptance.png&#34; width=&#34;75%&#34;&gt;
&lt;p&gt;and I had to immediately share this amazing news with my parents and my friends! I am extremely grateful for the Google Summer of Code (GSoC) opportunity for the many reasons. Firstly, I will learn more about open source development and probabilistic programming from well-established mentors. Secondly, this practical opportunity would complement well my summer studying for my PhD comprehensive exams. Last but not least, I&amp;rsquo;m being paid! ðŸ˜…&lt;/p&gt;
&lt;p&gt;Under the mentorship of 
&lt;a href=&#34;https://twitter.com/fonnesbeck&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Christopher Fonnesbeck&lt;/a&gt; and 
&lt;a href=&#34;https://www.austinrochford.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Austin Rochford&lt;/a&gt;, my GSoC project centers around extending the 
&lt;a href=&#34;https://github.com/pymc-devs/pymc3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyMC3&lt;/a&gt; package with a Dirichlet process submodule. PyMC3 is a probabilistic framework in Python that allows users to fit Bayesian models via MCMC sampling. There is a growing interest in Bayesian nonparametric methods for fitting statistical models without specifying its parametric form. Dirichlet processes can be used as priors for probability distributions themselves. How exactly? I shall find out in the next three weeks of community bonding!&lt;/p&gt;
&lt;p&gt;In the coming months, I look forward to getting to know the PyMC3 community, reading more about Bayesian nonparametric statistics and, above all, learning from my mentors. Stay tuned for more!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
